{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUVAZH2XHtmMt5TOp9/G0T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/deeplearning00/blob/main/FirstPrinciples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Principle Machine Learning Notebook.\n",
        "\n",
        "## Goal\n",
        "1. To create a Notebook that sums up my understanding of the topic.\n",
        "2. While creating this notebook gaining a deeper understanding.\n",
        "3. Getting familiar with Notebooks and what I can do in them."
      ],
      "metadata": {
        "id": "ikoC7S0sgCat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Machine learning?\n",
        "\n",
        "Machine learning models are trained to learn a mapping from input features to output targets. This is done by adjusting the model's parameters to minimize the difference between its predictions and the actual target values in the training data. This process is called optimization. W\n",
        "\n",
        "#### A model can be in one of three states:\n",
        "1. **Underfit**: An underfit model is too simple to capture the underlying patterns in the data. It performs poorly on both the training data and unseen data. This often happens because the model hasn't been trained for long enough or isn't complex enough. h\n",
        "2. **Well-generalized (Good Fit)**: A well-generalized model (or a model with a \"good fit\") learns the underlying patterns in the data without memorizing the noise. It performs well on the training data and, crucially, also performs well on unseen data. This ability to perform well on unseen data is called generalization, which is the core goal of machine learning.\n",
        "3.**Overfit**: An overfit model learns the training data too well, including the noise. While it might perform very well on the training data, it struggles to generalize to unseen data. This happens when the model is too complex or has been trained for too long, essentially memorizing the specifics of the training set instead of learning the general patterns.\n",
        "\n",
        "#### Training data may include noise, uncertainty, or rare features:\n",
        "**Noise**: refers to errors in the measurement of the features or targets. It's random and doesn't represent true underlying patterns.\n",
        "\n",
        "**Uncertainty**: represents the inherent randomness in the data itself. Even with the same input features, the true output might vary due to some underlying randomness in the process being modeled.\n",
        "\n",
        "**Rare Features:** are features that appear infrequently in the data. They can lead to spurious correlations, where the model learns to rely on these features for predictions, but those features don't generalize to new data. This is because the model mistakes a correlation in the small number of examples for a true underlying relationship"
      ],
      "metadata": {
        "id": "YeIpAkqhgaTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why does Generalization work?\n",
        "\n",
        "### Manifold Hypothesis\n",
        "Data lies on a low-dimensonal manifold (latent manifold) within the high-dimensonal space where it is encoded.\n",
        "\n",
        "** A manifold is a subspace of some parent space.\n",
        "\n",
        "**Implication**\n",
        "1. Machine learning models only have to fit this lower-dimensional manifold\n",
        "one example is a crunmpeled paper ball. The lower-dimensional manifold is the 2d surface if it.\n",
        "2. within these manifolds, its possible to **interpolate** between two inputs. Morph from one to the other via a connected, continous path.\n",
        "\n",
        "### Interpolation, source of Generalization\n",
        "\n",
        "Deep learning achieves generalization via interpolation on a learned approximation of the data manifold.\n",
        "\n",
        "1. local generalization : making sense of things that are very close to what youve seen before (interpolation)\n",
        "2. extreme genralization: abstraction, symbolic models, innate priors (What we humans can do as well)\n",
        "\n",
        "### Clarification\n",
        "\n",
        "The higher dimensional space exist outside of the model.\n",
        "The Model( weights, bias and the architecture) is looking for a lower dimensional pathway through that space hat explains the patterns in your data.\n",
        "The model curve is being fit to the data, gradually and smoothly over time.\n",
        "\n",
        "At one point it is a rough approximation of the latent space.\n",
        "\n",
        "### Fazit\n",
        "Deep learning is suited learning these latent manifolds, since they are smooth and continous in their structure just like the latent manifolds.\n",
        "\n",
        "Thus the genralization is more of the natural structure included in the Data.\n",
        "\n",
        "** The model will only be able to generalize where the data builds a manifold to interpolate between points **\n",
        "\n",
        "From this we can see that if our data sampling is noisy or not dense the loss function will be a bad approximation of the natural manifold.\n",
        "The loss function is taking the direct way between points.\n",
        "\n",
        "I want to end with this comment from Chollet\n",
        "\"The only thing you will find in your Deep learning model is what you put into it; The priors encoded in its architecture and the data it was trained on.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "re82KW65qf2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Workflow of Machine Learning\n",
        "\n",
        "|--Data---Assumption---Model Topology---Learning---Observing---Validating---Using the Model--|\n",
        "\n",
        "**The goal is to build a model, that generalized well - works on unseen data**\n",
        "\n",
        "\n",
        "There are key moments that are worth revisiting while builing.\n",
        "\n",
        "1.   The Assumptions about the data - this will determine the topology and thus the hypothesis space of the model.\n",
        "2.   The loss function - the model will take any shortcut it can to minimize.\n",
        "  Choosing the right loss function for the right problem(!check your Assumptions)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ThScAcKqYZkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Advice\n",
        "\n",
        "1. Preprocessing raw data before feeding it into a neural network.\n",
        "\n",
        "2. Normalizing data, mean shifting all data points into a range where the average sum is 0 (subtracting the mean and dividing by the standard diviation)\n",
        "\n",
        "3. Overfit the model to figure out the ideal training epochs. (Measure twice, cut once)\n",
        "\n",
        "4. Small training data, small model with only one or two layers. To avoid overfitting.\n",
        "\n",
        "5. Many categories for classfication, need larger layers. --> Information Bottleneck/Features are getting compressed\n",
        "\n",
        "6. working with little data, K-fold validation can help reliably evaluate your model"
      ],
      "metadata": {
        "id": "Makdbkm4HLde"
      }
    }
  ]
}