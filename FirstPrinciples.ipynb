{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ5TXDowFF94GD+jUhNh4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/deeplearning00/blob/main/FirstPrinciples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Principle Machine Learning Notebook.\n",
        "\n",
        "## Goal\n",
        "1. To create a Notebook that sums up my understanding of the topic.\n",
        "2. While creating this notebook gaining a deeper understanding.\n",
        "3. Getting familiar with Notebooks and what I can do in them."
      ],
      "metadata": {
        "id": "ikoC7S0sgCat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Machine learning?\n",
        "\n",
        "Machine learning models are trained to learn a mapping from input features to output targets. This is done by adjusting the model's parameters to minimize the difference between its predictions and the actual target values in the training data. This process is called optimization. W\n",
        "\n",
        "#### A model can be in one of three states:\n",
        "1. **Underfit**: An underfit model is too simple to capture the underlying patterns in the data. It performs poorly on both the training data and unseen data. This often happens because the model hasn't been trained for long enough or isn't complex enough. h\n",
        "2. **Well-generalized (Good Fit)**: A well-generalized model (or a model with a \"good fit\") learns the underlying patterns in the data without memorizing the noise. It performs well on the training data and, crucially, also performs well on unseen data. This ability to perform well on unseen data is called generalization, which is the core goal of machine learning.\n",
        "3.**Overfit**: An overfit model learns the training data too well, including the noise. While it might perform very well on the training data, it struggles to generalize to unseen data. This happens when the model is too complex or has been trained for too long, essentially memorizing the specifics of the training set instead of learning the general patterns.\n",
        "\n",
        "#### Training data may include noise, uncertainty, or rare features:\n",
        "**Noise**: refers to errors in the measurement of the features or targets. It's random and doesn't represent true underlying patterns.\n",
        "\n",
        "**Uncertainty**: represents the inherent randomness in the data itself. Even with the same input features, the true output might vary due to some underlying randomness in the process being modeled.\n",
        "\n",
        "**Rare Features:** are features that appear infrequently in the data. They can lead to spurious correlations, where the model learns to rely on these features for predictions, but those features don't generalize to new data. This is because the model mistakes a correlation in the small number of examples for a true underlying relationship"
      ],
      "metadata": {
        "id": "YeIpAkqhgaTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Workflow of Machine Learning\n",
        "\n",
        "|--Data---Assumption---Model Topology---Learning---Observing---Validating---Using the Model--|\n",
        "\n",
        "**The goal is to build a model, that generalized well - works on unseen data**\n",
        "\n",
        "\n",
        "There are key moments that are worth revisiting while builing.\n",
        "\n",
        "1.   The Assumptions about the data - this will determine the topology and thus the hypothesis space of the model.\n",
        "2.   The loss function - the model will take any shortcut it can to minimize.\n",
        "  Choosing the right loss function for the right problem(!check your Assumptions)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ThScAcKqYZkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Advice\n",
        "\n",
        "1. Preprocessing raw data before feeding it into a neural network.\n",
        "\n",
        "2. Normalizing data, mean shifting all data points into a range where the average sum is 0 (subtracting the mean and dividing by the standard diviation)\n",
        "\n",
        "3. Overfit the model to figure out the ideal training epochs. (Measure twice, cut once)\n",
        "\n",
        "4. Small training data, small model with only one or two layers. To avoid overfitting.\n",
        "\n",
        "5. Many categories for classfication, need larger layers. --> Information Bottleneck/Features are getting compressed\n",
        "\n",
        "6. working with little data, K-fold validation can help reliably evaluate your model"
      ],
      "metadata": {
        "id": "Makdbkm4HLde"
      }
    }
  ]
}